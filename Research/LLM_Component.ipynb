{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LLM Component\n",
    "Trying out the Hugging Face LLM model to use it as a Open-Ended Task Decorder."
   ],
   "id": "46d1485847015ed1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-04T18:21:47.235560Z",
     "start_time": "2024-11-04T18:21:31.157818Z"
    }
   },
   "source": [
    "from pycparser.ply.yacc import token\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Bart LLM is mainly used to generate text from given tokens or text. For the LLM task decoder of my Vision LLM it would be better to use a model that can generate a description or an answer based on a certain instruction. We can choose between the T5 or the Bert model. We will use the T5 model because it is specialized in giving descriptions and giving open-ended answers.",
   "id": "e86f4b20f9bed204"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:21:50.086272Z",
     "start_time": "2024-11-04T18:21:47.255576Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = T5Tokenizer.from_pretrained('t5-base')",
   "id": "e33a1a8d55307a3f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1349a74ba09d42adaafe04743cd18ac8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xande\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xande\\.cache\\huggingface\\hub\\models--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85e4085d7f584a0ab85ce612733143bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "843e02991c9846cd8f9ba85d1c94727f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:23:44.402030Z",
     "start_time": "2024-11-04T18:21:50.861827Z"
    }
   },
   "cell_type": "code",
   "source": "bart_model = T5ForConditionalGeneration.from_pretrained('t5-base')",
   "id": "cd414ccd2934552a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2dfeb04b47374fc69d226bfecf9edeba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d21554d1d8654430bdbbac37242705dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The tokenizer converts the textual format to tokens, a format the bart model can understand. \n",
    "The bart_model is the actual neural network that recieves the tokenized input and generates an output.\n",
    "\n",
    "Lets try with a basic query:"
   ],
   "id": "d2ebc95b0880bb7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:23:44.448104Z",
     "start_time": "2024-11-04T18:23:44.435016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_input = \"What is the most visited city in the world\"\n",
    "input_ids = tokenizer(text_input, return_tensors=\"pt\").input_ids\n",
    "input_ids"
   ],
   "id": "d937b596bb94002b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 363,   19,    8,  167, 5251,  690,   16,    8,  296,    1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:23:47.040224Z",
     "start_time": "2024-11-04T18:23:44.510884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_ids = bart_model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "output_ids"
   ],
   "id": "7659abedfbc8cee2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0, 32099,     8,   167,  5251,   690,    16,     8,   296,    58,\n",
       "           363,    19,     8,   167,  5251,   690,    16,     8,   296,    58,\n",
       "             1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:24:24.689150Z",
     "start_time": "2024-11-04T18:24:24.672983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "text_output"
   ],
   "id": "2aad74a2bbd56fd5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the most visited city in the world? What is the most visited city in the world?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is obvioudly not the answer we wanted, we probably need to fine-tune the model to get the desired output.",
   "id": "ff6f299abd414757"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:35:41.871973Z",
     "start_time": "2024-11-04T18:35:41.857145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_answer(text_input):\n",
    "    input_ids = tokenizer(text_input, return_tensors=\"pt\").input_ids\n",
    "    output_ids = bart_model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "    text_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return text_output"
   ],
   "id": "69d6c15a25f9a38b",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T18:43:32.126334Z",
     "start_time": "2024-11-04T18:43:31.915358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"question: What is the capital of Egypt?\"\n",
    "generate_answer(question)"
   ],
   "id": "2b9932324f62fa82",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cairo'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Given the complexity of the tasks my model will have to perform eventually, it will be better to use gpt-4 to generate the answers.",
   "id": "61b1bccdc412c6a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:13:32.855392Z",
     "start_time": "2024-11-04T19:13:32.851801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import openai\n",
    "import os"
   ],
   "id": "1120529a96534032",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:13:34.239367Z",
     "start_time": "2024-11-04T19:13:34.233469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# set root path and the .env as the evn file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "#%"
   ],
   "id": "8db12e56fdf40b0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:13:36.412940Z",
     "start_time": "2024-11-04T19:13:36.408731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "api = os.getenv('OPENAI_API_KEY')\n",
    "api"
   ],
   "id": "6a2c10e132d6e4e5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from openai import OpenAI\n",
    "openai.api_key = api\n",
    "\n",
    "model = \"gpt-4o\""
   ],
   "id": "f805d4bb78ca25e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T19:16:17.756655Z",
     "start_time": "2024-11-04T19:16:14.983596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = \"What are the top 3 visited cities in Europe?\"  # Example question\n",
    "\n",
    "client = OpenAI(api_key=api)\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=model,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that helps me with my questions.\"},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(\"Assistant: \" + response.choices[0].message.content)"
   ],
   "id": "95f0649f15a8125c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The top three most visited cities in Europe often include Paris, London, and Rome. These cities are renowned for their rich history, culture, cuisine, and numerous iconic landmarks that attract millions of tourists each year. However, exact rankings can fluctuate slightly depending on the specific year and the criteria used to measure tourism (such as international arrivals, overnight stays, or other metrics).\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This is a much better answer than the one we got from the T5 model. The GPT-4 model is much better at generating answers to open-ended questions. This is the model we will use for the LLM task decoder.",
   "id": "de9353f0e3ba6eff"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

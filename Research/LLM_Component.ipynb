{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LLM Component\n",
    "Trying out the Hugging Face LLM model to use it as a Open-Ended Task Decorder."
   ],
   "id": "46d1485847015ed1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-04T17:41:40.919165Z",
     "start_time": "2024-11-04T17:41:40.914928Z"
    }
   },
   "source": [
    "from pycparser.ply.yacc import token\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Bart LLM is mainly used to generate text from given tokens or text. For the LLM task decoder of my Vision LLM it would be better to use a model that can generate a description or an answer based on a certain instruction. We can choose between the T5 or the Bert model. We will use the T5 model because it is specialized in giving descriptions and giving open-ended answers.",
   "id": "e86f4b20f9bed204"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T17:43:42.896265Z",
     "start_time": "2024-11-04T17:43:42.863793Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = T5Tokenizer.from_pretrained('t5-base')",
   "id": "e33a1a8d55307a3f",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mT5Tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt5-base\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1651\u001B[0m, in \u001B[0;36mDummyObject.__getattribute__\u001B[1;34m(cls, key)\u001B[0m\n\u001B[0;32m   1649\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m key\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m key \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_from_config\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1650\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getattribute__\u001B[39m(key)\n\u001B[1;32m-> 1651\u001B[0m \u001B[43mrequires_backends\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_backends\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1639\u001B[0m, in \u001B[0;36mrequires_backends\u001B[1;34m(obj, backends)\u001B[0m\n\u001B[0;32m   1637\u001B[0m failed \u001B[38;5;241m=\u001B[39m [msg\u001B[38;5;241m.\u001B[39mformat(name) \u001B[38;5;28;01mfor\u001B[39;00m available, msg \u001B[38;5;129;01min\u001B[39;00m checks \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m available()]\n\u001B[0;32m   1638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m failed:\n\u001B[1;32m-> 1639\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(failed))\n",
      "\u001B[1;31mImportError\u001B[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T17:41:40.980684700Z",
     "start_time": "2024-11-04T17:26:15.515781Z"
    }
   },
   "cell_type": "code",
   "source": "bart_model = T5ForConditionalGeneration.from_pretrained('t5-base')",
   "id": "cd414ccd2934552a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fadac4c2f0e8416e976b95e83bcc4cb6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The tokenizer converts the textual format to tokens, a format the bart model can understand. \n",
    "The bart_model is the actual neural network that recieves the tokenized input and generates an output.\n",
    "\n",
    "Lets try with a basic query:"
   ],
   "id": "d2ebc95b0880bb7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T17:41:40.982700200Z",
     "start_time": "2024-11-04T17:30:22.841247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_input = \"What is the most visited city in the world\"\n",
    "input_ids = tokenizer(text_input, return_tensors=\"pt\").input_ids\n",
    "input_ids"
   ],
   "id": "d937b596bb94002b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 2264,   16,    5,  144, 3790,  343,   11,    5,  232,    2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T17:41:40.982700200Z",
     "start_time": "2024-11-04T17:33:06.307883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_ids = bart_model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "output_ids"
   ],
   "id": "7659abedfbc8cee2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,    0, 2264,   16,    5,  144, 3790,  343,   11,    5,  232,    2]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T17:41:40.983725500Z",
     "start_time": "2024-11-04T17:33:09.033334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "text_output"
   ],
   "id": "2aad74a2bbd56fd5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the most visited city in the world'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
